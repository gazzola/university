%==============================================================================
\chapter{Metodologia}\label{desenvolvimento}
%==============================================================================

Neste capítulo será explicado o método proposto para resolver o problema de \ac{pos} Tagging, juntamente com todas as técnicas utilizadas para seu funcionamento.

\section{Representação das palavras}

Seguiremos a ideia explorada em massa pela literatura de representar as palavras através de vetores reais com uma dimensão fixa $d$ definida pelo usuário. Isso será feito utilizando três estratégias já mencionadas: \ac{nlm}, \ac{sg} e \ac{glove}.

Além das \textit{word embeggins}, utilizaremos outras duas \textit{features} importantes no contexto de \ac{pos} Tagging, como capitalização que consegue, na maioria das vezes, distinguir nomeações, e também prefixos, que pode ser usada para distinguir medidas de tempo, velocidade, etc. 

\section{Pontuações para estrutura gramatical}

Em ordem de classificar palavras em uma sentença, o etiquetador obtém uma janela de palavras de tamanho fixo a cada momento, e transforma as palavras em vetores de \textit{features}, que são então passadas para uma rede neural descrita em \cite{collobert2008unified}. A rede atribui para cada classe gramatical $c \in \gamma$ uma pontuação. A etapa de gerar pontuações para a palavra ocorre no mesmo momento que o treinamento do modelo. A saída para toda a sentença é então passada para o algoritmo de Viterbi \cite{viterbi1967error}, que realiza uma predição estruturada em um tempo polinomial.

Seguiremos a ideia apresentada em \cite{dos2014training} para realizar as pontuações. Nela, a ideia é de que a classe gramatical de uma palavra depende fortemente das palavras vizinhas, o que é verdade para várias aplicações de \ac{pln}, incluindo \ac{pos} Tagging.

Dada uma janela com $t$ palavras $\{w_1, w_2, ..., w_t\}$, que foi foram transformadas para sua representação vetorial $\{v_1, v_2, ..., v_n\}$, para computar a n-ésima palavra, é centralizada a janela em $n$ e concatena-se todas as palavras da metade à esquerda e da metade à direita em um novo vetor $V_n$ de dimensão $t * d$. Para palavras no começo ou no fim da sentença, é usado vetores com valores fixos para preencher o espaço vazio na janela de palavras. A \autoref{eq:janeladevets} demonstra isso.

\begin{equation} \label{eq:janeladevets}
V_n = \big\{ v_{n - (t-1)/2}, ..., v_n, ..., v_{{n + (t-1)/2}} \big\}
\end{equation}

O novo vetor $V_n$ é então passado para uma rede neural com múltiplas camadas, que computa as pontuações $s_c(V_n)$ para cada classe gramatical $c$ da palavra no meio da janela. 

Seguimos a ideia apresentada em \cite{collobert2011natural}, onde é feito um esquema de predição que leva em consideração a estrutura gramatical. O método usa uma pontuação de transição $A_{c,d}$ para ir de uma classe $c \in \gamma$ para uma classe $d \in \gamma$ conforme a sequência das palavras. A estrutura $A$ consegue armazenar informações importantes como ``após um pronome é bastante provável que há um verbo''. Depois que a rede produz a pontuação para todas as palavras, a pontuação final de uma sequência de classes gramáticas $c_1^t$ para uma sequência de palavras $w_1^t$, é dada pela \autoref{eq:pontuacaofinal}. 

\begin{equation} \label{eq:pontuacaofinal}
S(w_1^t, c_1^t) = \sum\limits_{i=1}^{t} \Big(s_{c_i}(V_i) + A_{c_{i-1}, c_{i}} \Big)
\end{equation}

Após computar isso para cada palavra na sentença, a classe gramatical é prevista através do algoritmo de Viterbi.


\section{Treinamento}

Para treinar a rede neural, será seguido um modelo de aprendizagem guiada por palavras mais fáceis \cite{shen2007guided}.

